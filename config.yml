token: { bpe_model_path: ./data/joint_enfr.30k.bpe, mode: conservative, joiner_annotate: true }
vocab: ./data/enfr.vocab
num_layers: 6
hidden_size: 512
feedforward_size: 2048
num_heads: 8
emb_size: 512
dropout: 0.1
cuda: false
learning_rate: 0.0001
beta1: 0.9
beta2: 0.98
eps: 0.000000001
warmup_steps: 10000
factor: 1.0
label_smoothing: 0.1

